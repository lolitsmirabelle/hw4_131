---
title: "Homework 4"
output: html_document
date: '2022-04-29'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidymodels)
library(ISLR) 
library(ISLR2) 
library(discrim)
library(poissonreg)
library(corrr)
library(klaR) # for naive bayes
tidymodels_prefer()
```


## Question 1

```{r}
titanic <- read.csv("~/Downloads/homework-4/data/titanic.csv")

titanic$survived <- factor(titanic$survived,levels=c('Yes', 'No'))
titanic$pclass <- factor(titanic$pclass)

set.seed(2424)

titanic_split <- initial_split(titanic, prop = 0.80,
                                strata = survived)

titanic_train <- training(titanic_split) 

titanic_test <- testing(titanic_split)
```


## Question 2 

```{r}
titanic_recipe <- 
  recipe(survived ~ ., data = titanic_train%>%select(survived, sex, age, sib_sp, pclass, fare, parch)) %>% 
  step_impute_linear(age) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(terms = ~ starts_with("sex"):fare + age:fare) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())

lm_spec <- linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm")

poly_rec <- recipe(survived ~ ., data = titanic_train%>%select(survived, sex, age, sib_sp, pclass, fare, parch)) %>%
  step_poly(., degree = 2)

poly_wf <- workflow() %>%
  add_recipe(poly_rec) %>%
  add_model(lm_spec)

poly_tuned_rec <- recipe(survived ~ ., data = titanic_train%>%select(survived, sex, age, sib_sp, pclass, fare, parch)) %>%
  step_poly(., degree = tune())

poly_tuned_wf <- workflow() %>%
  add_recipe(poly_tuned_rec) %>%
  add_model(lm_spec)

titanic_folds <- vfold_cv(titanic_train, v = 10)
titanic_folds

```


## Question 3 
In question 2, k-fold cross validation is also known as hyperparameter tuning and involves randomly k-fold CV dividing the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining k âˆ’ 1 folds. We should use this metric because it takes into account the models performance across the entire data set. Training on the entire training set would be 2-fold cross validation.

## Question 4 
```{r}
# logistic regression model
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(titanic_recipe)

# lda model
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(titanic_recipe)

# qda model 
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")

qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(titanic_recipe)
```
The total amount of folds is 30.


## Question 5
```{r, eval = FALSE}
log_fit_rs <- log_wkflow %>% fit_resamples(titanic_folds)
lda_fit_rs <- lda_wkflow %>% fit_resamples(titanic_folds)
qda_fit_rs <- qda_wkflow %>% fit_resamples(titanic_folds)
```

```{r,include=FALSE}
load("model_fits.rda")
```


## Question 6 
```{r}
collect_metrics(log_fit_rs)
collect_metrics(lda_fit_rs)
collect_metrics(qda_fit_rs)
```
From the above, we see that logistic regression yields the best result. This is because it is easier to implement, interpret, and very efficient to train. If the number of observations is lesser than the number of features, Logistic Regression should not be used, otherwise, it may lead to overfitting. It makes no assumptions about distributions of classes in feature space.

## Question 7
```{r}
best_model_fit <- log_wkflow %>% fit(titanic_train)
```


## Question 8
```{r}
log_test_acc <- predict(best_model_fit, new_data = titanic_test)%>%
  bind_cols(titanic_test%>%dplyr::select(survived))%>%
  accuracy(truth = survived, estimate = .pred_class)
log_test_acc
```

From the above, we observe that the accuracy on the testing data is higher than the accuracy on the folds. 